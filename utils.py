"""
Utility functions for the GenAI Image Comparison project.
"""

import os
import json
import glob
import numpy as np
from PIL import Image
from datetime import datetime
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

from config import REPORTS_DIR, OUTPUT_DIR


def load_images_from_directory(directory, pattern="*.png"):
    """
    Load all images from a directory that match the given pattern.
    
    Args:
        directory: Directory path to load images from
        pattern: Glob pattern to match files
        
    Returns:
        list: List of PIL Image objects
    """
    images = []
    for img_path in glob.glob(os.path.join(directory, pattern)):
        try:
            img = Image.open(img_path).convert('RGB')
            images.append(img)
        except Exception as e:
            print(f"Error loading image {img_path}: {e}")
    
    return images


def save_human_evaluation_results(evaluations, filename=None):
    """
    Save human evaluation results to a JSON file.
    
    Args:
        evaluations: Dictionary containing evaluation results
        filename: Optional filename for the saved results
        
    Returns:
        str: Path to the saved file
    """
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"human_evaluation_{timestamp}.json"
    
    file_path = os.path.join(REPORTS_DIR, filename)
    
    with open(file_path, 'w') as f:
        json.dump(evaluations, f, indent=2)
    
    return file_path


def load_human_evaluation_results(file_path):
    """
    Load human evaluation results from a JSON file.
    
    Args:
        file_path: Path to the JSON file
        
    Returns:
        dict: Human evaluation results
    """
    with open(file_path, 'r') as f:
        evaluations = json.load(f)
    
    return evaluations


def calculate_inter_rater_reliability(evaluations):
    """
    Calculate inter-rater reliability between different evaluators.
    
    Args:
        evaluations: Dictionary with evaluator names as keys and lists of evaluations as values
        
    Returns:
        dict: Reliability scores between pairs of evaluators
    """
    reliability_scores = {}
    evaluators = list(evaluations.keys())
    
    for i, eval1 in enumerate(evaluators):
        for j, eval2 in enumerate(evaluators[i+1:], i+1):
            scores1 = [e['overall_quality'] for e in evaluations[eval1]]
            scores2 = [e['overall_quality'] for e in evaluations[eval2]]
            
            # Ensure we're comparing the same images
            if len(scores1) != len(scores2):
                print(f"Warning: Evaluators {eval1} and {eval2} rated different numbers of images")
                continue
            
            correlation, p_value = pearsonr(scores1, scores2)
            reliability_scores[f"{eval1}_vs_{eval2}"] = {
                'correlation': correlation,
                'p_value': p_value
            }
    
    return reliability_scores


def create_image_grid(images, titles=None, cols=3, figsize=(15, 10)):
    """
    Create a grid of images for visual comparison.
    
    Args:
        images: List of PIL Image objects or paths to images
        titles: Optional list of titles for each image
        cols: Number of columns in the grid
        figsize: Figure size (width, height) in inches
        
    Returns:
        matplotlib.figure.Figure: The figure containing the grid
    """
    # Convert paths to PIL Images if needed
    processed_images = []
    for img in images:
        if isinstance(img, str):
            processed_images.append(Image.open(img).convert('RGB'))
        else:
            processed_images.append(img)
    
    # Calculate grid dimensions
    n_images = len(processed_images)
    rows = (n_images + cols - 1) // cols
    
    fig = plt.figure(figsize=figsize)
    
    for i, img in enumerate(processed_images):
        ax = fig.add_subplot(rows, cols, i + 1)
        ax.imshow(np.array(img))
        ax.axis('off')
        
        if titles and i < len(titles):
            ax.set_title(titles[i])
    
    plt.tight_layout()
    return fig


def compare_models_visually(prompt, model_names, output_dir=OUTPUT_DIR):
    """
    Create a visual comparison of images generated by different models for the same prompt.
    
    Args:
        prompt: The prompt used to generate the images
        model_names: List of model names to compare
        output_dir: Directory containing the generated images
        
    Returns:
        matplotlib.figure.Figure: The figure containing the comparison
    """
    # Find images for this prompt from each model
    images = []
    titles = []
    
    for model in model_names:
        # Find image files that match this model and contain the prompt hash
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:8]
        pattern = f"{model}_*_{prompt_hash}.png"
        matching_files = glob.glob(os.path.join(output_dir, pattern))
        
        if matching_files:
            images.append(matching_files[0])
            titles.append(model)
    
    # Create and return the comparison grid
    return create_image_grid(images, titles)


def send_completion_notification(report_path):
    """
    Send a notification that the evaluation has completed.
    This is a placeholder function - implement according to your notification needs.
    
    Args:
        report_path: Path to the generated report
    """
    print(f"Evaluation completed! Report available at: {report_path}")
    # Implement your notification logic here (email, Slack, etc.)


def send_error_notification(error_message):
    """
    Send a notification about an error during evaluation.
    This is a placeholder function - implement according to your notification needs.
    
    Args:
        error_message: The error message to send
    """
    print(f"Error during evaluation: {error_message}")
    # Implement your notification logic here (email, Slack, etc.)
